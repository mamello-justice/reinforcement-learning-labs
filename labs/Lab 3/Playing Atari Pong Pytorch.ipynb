{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd22t268l6tX"
      },
      "source": [
        "# Reinforcement Learning (RL)\n",
        "\n",
        "## Human-level control through deep rl [[Paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)]\n",
        "## Playing Atari with Deep RL [[Paper](https://arxiv.org/abs/1312.5602)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7CzsZ9YrClJ2",
        "outputId": "cb8e9d6a-8596-4432-9b18-12592ba2d7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gym==0.26.1 --quiet\n",
        "%pip install gym[atari] --quiet\n",
        "%pip install autorom[accept-rom-license] --quiet\n",
        "\n",
        "%pip install comet_ml --quiet\n",
        "%pip install tensorboardX --quiet\n",
        "%pip install onnx --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHgCzH2mJxP"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: Comet API key is valid\n"
          ]
        }
      ],
      "source": [
        "import comet_ml\n",
        "comet_ml.init(project_name='pong-dqn-ddqn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mPuZ-jKOmORc",
        "outputId": "2dcd1dc4-ea8c-462a-dcb2-654e9ee98aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gym==0.26.1\n",
            "numpy==1.23.3\n",
            "torch==1.12.1+cpu\n",
            "tensorboardX==2.5.1\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorboardX as tb\n",
        "\n",
        "import gym\n",
        "from gym import spaces, wrappers\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import trange\n",
        "except Exception:\n",
        "    trange = range\n",
        "\n",
        "print(f\"gym=={gym.__version__}\")\n",
        "print(f\"numpy=={np.__version__}\")\n",
        "print(f\"torch=={torch.__version__}\")\n",
        "print(f\"tensorboardX=={tb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6tzupwRzjCu"
      },
      "source": [
        "## Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IkEUiiFv-uFI"
      },
      "outputs": [],
      "source": [
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(\n",
        "                1, self.noop_max + 1\n",
        "            )  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs, info = None, None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _, info = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs, info = self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _, info = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _, info = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _, info = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs, info\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros(\n",
        "            (2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        ob, _ = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, trunc, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, trunc, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpGXom6qK8p"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "t47LFudfqM2y"
      },
      "outputs": [],
      "source": [
        "class TrainingEnvironment:\n",
        "    def __init__(self, env_name, seed, steps):       \n",
        "        assert \"NoFrameskip\" in env_name, \"Require environment with no frameskip\"\n",
        "\n",
        "        self.seed = seed\n",
        "        self.steps = steps\n",
        "\n",
        "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "        self.env.seed(seed)\n",
        "\n",
        "        self.env = wrappers.RecordVideo(self.env, \"videos\")\n",
        "        self.env = NoopResetEnv(self.env, noop_max=30)\n",
        "        self.env = MaxAndSkipEnv(self.env, skip=4)\n",
        "        self.env = EpisodicLifeEnv(self.env)\n",
        "        self.env = FireResetEnv(self.env)\n",
        "        self.env = ClipRewardEnv(self.env)\n",
        "        self.env = WarpFrame(self.env)\n",
        "        self.env = PyTorchFrame(self.env)\n",
        "\n",
        "    def unwrap(self):\n",
        "        return self.env\n",
        "    \n",
        "    def trigger(self, x):\n",
        "        if x % 100 == 0:\n",
        "            print(x)\n",
        "        return x > self.steps - 600\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"seed = {self.seed}\\nstates = {self.env.observation_space.shape}\\nactions = {self.env.action_space.n}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_9JYFzzoTI"
      },
      "source": [
        "## Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4IphrCLm-o5J"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, size):\n",
        "        self._maxsize = size\n",
        "        self._mem_counter = 0\n",
        "        self._state_shape = env.observation_space.shape\n",
        "        self._num_actions = env.action_space.n\n",
        "\n",
        "        self.states = np.zeros((self._maxsize, *self._state_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(self._maxsize, dtype=np.int64)\n",
        "        self.rewards = np.zeros(self._maxsize, dtype=np.float32)\n",
        "        self.next_states = np.zeros((self._maxsize, *self._state_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros(self._maxsize, dtype=np.float32)\n",
        "\n",
        "    def __call__(self, indices):\n",
        "        return self.states[indices],\\\n",
        "            self.actions[indices],\\\n",
        "            self.rewards[indices],\\\n",
        "            self.next_states[indices],\\\n",
        "            self.dones[indices]\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        next_index = self._mem_counter % self._maxsize\n",
        "\n",
        "        self.states[next_index] = np.array(state)\n",
        "        self.actions[next_index] = action\n",
        "        self.rewards[next_index] = reward\n",
        "        self.next_states[next_index] = np.array(next_state)\n",
        "        self.dones[next_index] = float(done)\n",
        "\n",
        "        self._mem_counter += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        memory = min(self._mem_counter, self._maxsize) - 1\n",
        "        assert self._mem_counter >= batch_size, f\"memory={memory} must have at least batch_size={batch_size} frames\"\n",
        "\n",
        "        indices = np.random.randint(0, memory, size=batch_size)\n",
        "\n",
        "        return self(indices)\n",
        "\n",
        "    def state_dict(self):\n",
        "        dict = {\n",
        "            'maxsize': self._maxsize,\n",
        "            'mem_counter': self._mem_counter,\n",
        "            'state_shape': self._state_shape,\n",
        "            'num_actions': self._num_actions,\n",
        "            'states': self.states,\n",
        "            'actions': self.actions,\n",
        "            'rewards': self.rewards,\n",
        "            'next_state': self.next_states,\n",
        "            'dones': self.dones,\n",
        "        }\n",
        "\n",
        "        return dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._maxsize = state_dict['maxsize']\n",
        "        self._mem_counter = state_dict['mem_counter']\n",
        "        self._state_shape = state_dict['state_shape']\n",
        "        self._num_actions = state_dict['num_actions']\n",
        "\n",
        "        self.states = np.array(state_dict['states'], dtype=np.float32)\n",
        "        self.actions = np.array(state_dict['actions'], dtype=np.int64)\n",
        "        self.rewards = np.array(state_dict['rewards'], dtype=np.float32)\n",
        "        self.next_states = np.array(state_dict['next_states'], dtype=np.float32)\n",
        "        self.dones = np.array(state_dict['dones'], dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dB0U6g8z0Fv"
      },
      "source": [
        "## DQN Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-justdcy-c1R"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, env, memory_size, use_double_dqn, lr, batch_size, gamma, device, log_dir, log_weights):\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.device = device\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        self.episode_rewards = [0.0]\n",
        "        self.should_log_weights = log_weights\n",
        "\n",
        "        # Tensorboard\n",
        "        self.tb_w = tb.SummaryWriter(comet_config={\"disabled\": False})\n",
        "\n",
        "        # Q-networks\n",
        "        self.Q = self._build_model().to(self.device)\n",
        "        self.Q_target = self._build_model().to(self.device)\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Adam optimizer for Q network\n",
        "        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=lr)\n",
        "\n",
        "        # Replay buffer/memory\n",
        "        self.memory = ReplayBuffer(env=env, size=memory_size)\n",
        "\n",
        "        # Indexing\n",
        "        self.num_optims = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        # 1 channel -> 16 features -> 32 features -> 32*9*9=2592 features -> 256 -> # actions\n",
        "        return nn.Sequential(nn.Conv2d(1, 16, kernel_size=8, stride=4, padding=0),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Flatten(),\n",
        "                             nn.Linear(32*9*9, 256),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(256, self.num_actions))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # Convert to tensor with added dimension\n",
        "        state = torch.tensor([state], dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Normalize state\n",
        "        state = state / 255\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Greedy action (max)\n",
        "            return self.Q(state).argmax(dim=1, keepdim=True).item()\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        \n",
        "        # Alternative to .gather()\n",
        "        indices = np.arange(self.batch_size)\n",
        "        \n",
        "        states = torch.tensor(batch[0], device=self.device)\n",
        "        next_states = torch.tensor(batch[3], device=self.device)\n",
        "\n",
        "        # Normalize memory states\n",
        "        states = states / 255\n",
        "        next_states = next_states / 255\n",
        "\n",
        "        # Targets\n",
        "        with torch.no_grad():  # Not used in gradient calculation\n",
        "            if self.use_double_dqn:\n",
        "                # Get action using online Q\n",
        "                target_actions = self.Q(next_states)\\\n",
        "                    .argmax(dim=1, keepdim=True)\\\n",
        "                    .squeeze()\n",
        "\n",
        "                # Get target Q values using actions above\n",
        "                target_values = self.Q_target(next_states)[indices, target_actions]\n",
        "\n",
        "            else:\n",
        "                # torch.max() -> tuple(Tensor[values], Tensor[indices])\n",
        "                target_values = self.Q_target(next_states)\\\n",
        "                    .max(dim=1, keepdim=True)[0]\\\n",
        "                    .squeeze()\n",
        "\n",
        "            rewards = torch.tensor(batch[2], device=self.device)\n",
        "            dones = torch.tensor(batch[4], device=self.device)\n",
        "            targets = rewards + \\\n",
        "                self.gamma * target_values * (1 - dones)\n",
        "\n",
        "        # Online\n",
        "        actions = torch.tensor(batch[1], device=self.device)\n",
        "        values = self.Q(states)[indices, actions]\n",
        "\n",
        "        # Loss\n",
        "        loss = self.criterion(targets, values).to(self.device)\n",
        "\n",
        "        # Gradient descent\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Log to tensorboard\n",
        "        self.tb_w.add_scalar(\"data/loss\", loss.item(), self.num_optims)\n",
        "\n",
        "        if self.should_log_weights:\n",
        "            # Expensive comp\n",
        "            self.log_weights()\n",
        "\n",
        "        self.num_optims += 1\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Rewards\n",
        "        self.episode_rewards[-1] += reward\n",
        "\n",
        "        if done:\n",
        "            self.episode_rewards.append(0.0)\n",
        "\n",
        "    def num_episodes(self):\n",
        "        return len(self.episode_rewards)\n",
        "    \n",
        "    \n",
        "    def log_onnx(self, dummy):\n",
        "        model = self._build_model()\n",
        "        \n",
        "        dummy = dummy / 255\n",
        "        dummy = (torch.tensor([dummy], dtype=torch.float32),)\n",
        "        \n",
        "        filename = 'data/model.onnx'\n",
        "        input_axis = { 0: \"batch_size\", 1: \"channel\", 2: \"height\", 3: \"width\"}\n",
        "        output_axis = { 0: \"action\" }\n",
        "        \n",
        "        torch.onnx.export(model, \n",
        "                          dummy, \n",
        "                          filename,\n",
        "                          export_params=True,\n",
        "                          opset_version=10,\n",
        "                          do_constant_folding=True,\n",
        "                          input_names=[\"input\"],\n",
        "                          output_names=[\"output\"],\n",
        "                          dynamic_axes={ \"input\": input_axis, \"output\": output_axis })\n",
        "        \n",
        "        self.tb_w.add_onnx_graph(filename)\n",
        "\n",
        "    def log_weights(self):\n",
        "        for name, weight in self.Q.named_parameters():\n",
        "            self.tb_w.add_histogram('data/weights/Q-%s' % name, weight, self.num_optims)\n",
        "            self.tb_w.add_histogram('data/weights/Q-%s.grad' % name, weight, self.num_optims)\n",
        "\n",
        "        for name, weight in self.Q_target.named_parameters():\n",
        "            self.tb_w.add_histogram('data/weights/Q-target-%s' % name, weight, self.num_optims)\n",
        "            self.tb_w.add_histogram('data/weights/Q-target-%s' % name, weight, self.num_optims)\n",
        "\n",
        "    def log(self, t):\n",
        "        self.tb_w.add_scalar(\"data/episodes\", self.num_episodes(), t)\n",
        "\n",
        "        name = \"data/weights/rewards\"\n",
        "        mean_100ep_reward = round(np.mean(self.episode_rewards[-101:-1]), 1)\n",
        "        self.tb_w.add_scalar(name, mean_100ep_reward, t)\n",
        "\n",
        "    def save(self, path):\n",
        "        print('saving checkpoint...')\n",
        "\n",
        "        checkpoint = {\n",
        "            'Q': self.Q.state_dict(),\n",
        "            'Q_target': self.Q_target.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'loss': self.criterion,\n",
        "            'memory': self.memory.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        print('loading checkpoints...')\n",
        "\n",
        "        checkpoint = torch.load(path)\n",
        "\n",
        "        self.Q.load_state_dict(checkpoint.get('Q'), map_location=self.device)\n",
        "        self.Q.train()\n",
        "\n",
        "        self.Q_target.load_state_dict(\n",
        "            checkpoint.get('Q_target'),\n",
        "            map_location=self.device)\n",
        "        self.Q_target.train()\n",
        "\n",
        "        self.optimizer.load_state_dict(checkpoint.get('optimizer'))\n",
        "\n",
        "        self.criterion = checkpoint.get('loss')\n",
        "\n",
        "        self.memory.load_state_dict(checkpoint.get('memory'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "args_vars = {\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'log_weights': False,\n",
        "    'log_dir': None,\n",
        "    'stat_freq': 10,\n",
        "    'out': 'data/model.pth',\n",
        "    'checkpoint_freq': int(1e4),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyper_params = {\n",
        "    \"seed\": 42,\n",
        "    \"env\": \"PongNoFrameskip-v4\",\n",
        "    \"use-double-dqn\": True,\n",
        "    \"replay-buffer-size\": int(5e3),\n",
        "    \"batch-size\": 256,\n",
        "    \"num-steps\": int(1e4),\n",
        "    \"learning-starts\": int(1e3),\n",
        "    \"learning-freq\": 4,\n",
        "    \"target-update-freq\": int(5e2),\n",
        "    \"learning-rate\": int(1e-4),\n",
        "    \"discount-factor\": 0.99,\n",
        "    \"eps-start\": 1.0,\n",
        "    \"eps-end\": 0.01,\n",
        "    \"eps-fraction\": 0.1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqnp_A4HshEi"
      },
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "527702ac599a45f181663eea8ad78e86",
            "25eb2d6e212b4b669151ac8055885b84",
            "5173742c3b2a4337940e66c7eb303157",
            "685953149a91470ab126d945bfa054a6",
            "7f7ba58234a04495a007d0daf8c88fa8",
            "21a6648456fa41acb7142db387443e6f",
            "9ef52bde73ac4820bb81182c618c0dbd",
            "fbbf068fae424a2c8f5655884d7d885d",
            "23b7899fcdab4cfa9d6895ebceedc22e",
            "3130bb47ab58470d9a888ec11553b035",
            "6b814c4023f843de8948d77438f1d693"
          ]
        },
        "id": "FhZGtULnsjSA",
        "outputId": "f4dfae22-a816-4994-f23a-7a6b9617e635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: Comet API key is valid\n",
            "WARNING:tensorboardX.comet_utils:You have already created a comet                                         experiment manually, which might                                         cause clashes\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/mamello-justice/pong-dqn-ddqn/22dce2d95a9746438ee698e4b8b0a7ab\n",
            "COMET INFO:   Others:\n",
            "COMET INFO:     Created from : tensorboardX\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     batch-size         : 256\n",
            "COMET INFO:     discount-factor    : 0.99\n",
            "COMET INFO:     env                : PongNoFrameskip-v4\n",
            "COMET INFO:     eps-end            : 0.01\n",
            "COMET INFO:     eps-fraction       : 0.1\n",
            "COMET INFO:     eps-start          : 1.0\n",
            "COMET INFO:     learning-freq      : 4\n",
            "COMET INFO:     learning-rate      : 0\n",
            "COMET INFO:     learning-starts    : 1000\n",
            "COMET INFO:     num-steps          : 10000\n",
            "COMET INFO:     replay-buffer-size : 5000\n",
            "COMET INFO:     seed               : 42\n",
            "COMET INFO:     target-update-freq : 500\n",
            "COMET INFO:     use-double-dqn     : True\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     asset                    : 1 (2.57 MB)\n",
            "COMET INFO:     environment details      : 1\n",
            "COMET INFO:     filename                 : 1\n",
            "COMET INFO:     git metadata             : 1\n",
            "COMET INFO:     git-patch (uncompressed) : 1 (59.50 KB)\n",
            "COMET INFO:     installed packages       : 1\n",
            "COMET INFO:     notebook                 : 1\n",
            "COMET INFO:     source_code              : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/mamello-justice/pong-dqn-ddqn/60e589451cb540118f24be21fba03b12\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input\n",
            "output\n",
            "input.1\n",
            "input.4\n",
            "input.8\n",
            "onnx::Flatten_12\n",
            "onnx::Gemm_13\n",
            "input.12\n",
            "onnx::Gemm_15\n",
            "output\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [03:22<00:00, 49.40it/s]\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/mamello-justice/pong-dqn-ddqn/60e589451cb540118f24be21fba03b12\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     Explore_time         : 1\n",
            "COMET INFO:     data/episodes        : 10\n",
            "COMET INFO:     data/loss [2249]     : (4.759837611345574e-05, 0.07420691847801208)\n",
            "COMET INFO:     data/weights/rewards : -21.0\n",
            "COMET INFO:     loss [225]           : (0.008951930329203606, 0.06974563747644424)\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     batch-size         : 256\n",
            "COMET INFO:     discount-factor    : 0.99\n",
            "COMET INFO:     env                : PongNoFrameskip-v4\n",
            "COMET INFO:     eps-end            : 0.01\n",
            "COMET INFO:     eps-fraction       : 0.1\n",
            "COMET INFO:     eps-start          : 1.0\n",
            "COMET INFO:     learning-freq      : 4\n",
            "COMET INFO:     learning-rate      : 0\n",
            "COMET INFO:     learning-starts    : 1000\n",
            "COMET INFO:     num-steps          : 10000\n",
            "COMET INFO:     replay-buffer-size : 5000\n",
            "COMET INFO:     seed               : 42\n",
            "COMET INFO:     target-update-freq : 500\n",
            "COMET INFO:     use-double-dqn     : True\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     asset                    : 1 (2.57 MB)\n",
            "COMET INFO:     environment details      : 1\n",
            "COMET INFO:     filename                 : 1\n",
            "COMET INFO:     git metadata             : 1\n",
            "COMET INFO:     git-patch (uncompressed) : 1 (20.28 KB)\n",
            "COMET INFO:     installed packages       : 1\n",
            "COMET INFO:     model graph              : 1\n",
            "COMET INFO:     notebook                 : 1\n",
            "COMET INFO:     source_code              : 1\n",
            "COMET INFO: ---------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total episodes:  14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
            "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
            "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
            "COMET INFO: The Python SDK has 10800 seconds to finish before aborting...\n",
            "COMET INFO: Still uploading 2 file(s), remaining 56.98 KB/2.78 MB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving checkpoint...\n"
          ]
        }
      ],
      "source": [
        "def get_epsilon_threshold(t, hyper_params):\n",
        "    eps_end = hyper_params['eps-end']\n",
        "    eps_start = hyper_params['eps-start']\n",
        "    eps_fraction = hyper_params['eps-fraction']\n",
        "    steps = hyper_params['num-steps']\n",
        "    \n",
        "    fraction = min(1.0, float(t) / (eps_fraction * float(steps)))\n",
        "    return eps_start + fraction * (eps_end - eps_start)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # default_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # args_vars = setup_args(default_device=default_device)\n",
        "    # hyper_params = args_to_hyper_params(args_vars)\n",
        "    \n",
        "    device = torch.device(args_vars.get(\"device\"))\n",
        "    print(\"device = %s\" % device)\n",
        "\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "\n",
        "    train_env = TrainingEnvironment(env_name=hyper_params['env'],\n",
        "                                    seed=hyper_params['seed'],\n",
        "                                    steps=hyper_params['num-steps'])\n",
        "\n",
        "    env = train_env.unwrap()\n",
        "\n",
        "    agent = DQNAgent(env=env, \n",
        "                     memory_size=hyper_params['replay-buffer-size'], \n",
        "                     use_double_dqn=hyper_params['use-double-dqn'],\n",
        "                     lr=hyper_params['learning-rate'],\n",
        "                     batch_size=hyper_params['batch-size'],\n",
        "                     gamma=hyper_params['discount-factor'],\n",
        "                     device=device,\n",
        "                     log_dir=args_vars['log_dir'],\n",
        "                     log_weights=args_vars['log_weights'])\n",
        "\n",
        "    agent.tb_w.add_hparams(hparam_dict=hyper_params, metric_dict={})\n",
        "    \n",
        "    dummy, _ = env.reset()\n",
        "    agent.log_onnx(dummy)\n",
        "\n",
        "    in_file = args_vars.get('in')\n",
        "    if in_file is not None:\n",
        "        try:\n",
        "            agent.load(in_file)\n",
        "        except FileNotFoundError:\n",
        "            print(\"model file not found: %s\" % in_file)\n",
        "\n",
        "\n",
        "    out_file = args_vars.get('out')\n",
        "    checkpoint_freq = args_vars['checkpoint_freq']\n",
        "    try:\n",
        "        state, _ = env.reset()\n",
        "        for t in trange(hyper_params['num-steps']):\n",
        "            epsilon_threshold = get_epsilon_threshold(t, hyper_params)\n",
        "\n",
        "            #  select random action if sample is less equal than eps_threshold\n",
        "            if (t <= hyper_params['learning-starts'] or random.random() <= epsilon_threshold):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = agent.act(state)\n",
        "\n",
        "            # take step in env\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "            agent.remember(state=state,\n",
        "                            action=action,\n",
        "                            reward=reward,\n",
        "                            next_state=next_state,\n",
        "                            done=float(done))\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                state, _ = env.reset()\n",
        "                \n",
        "                if (agent.num_episodes() % args_vars.get('stat_freq') == 0):\n",
        "                    explore_time = int(100 * epsilon_threshold)\n",
        "                    agent.log(t)\n",
        "                    agent.tb_w.add_scalar(\"Explore time\", explore_time, t)\n",
        "\n",
        "            if t > hyper_params['learning-starts']:\n",
        "                if t % hyper_params['learning-freq'] == 0:\n",
        "                    agent.optimise_td_loss()\n",
        "\n",
        "                if t % hyper_params['target-update-freq'] == 0:\n",
        "                    agent.update_target_network()\n",
        "                    \n",
        "            if (out_file is not None and checkpoint_freq is not None and t % checkpoint_freq == 0):\n",
        "                agent.save(out_file)\n",
        "                    \n",
        "        print(\"total episodes: \", agent.num_episodes())\n",
        "\n",
        "    finally:\n",
        "        agent.tb_w.close()\n",
        "        if out_file is not None:\n",
        "            agent.save(out_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mWHgCzH2mJxP",
        "lF17Bj-8nfSk",
        "e6tzupwRzjCu",
        "rZpGXom6qK8p",
        "9WmQIaalzuev",
        "8dB0U6g8z0Fv",
        "Wqnp_A4HshEi"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "98273bd8a540ecd20beace9e7ace114571d037c3f5382e4595714f5a9fa1d3c0"
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 ('rl_lab_3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21a6648456fa41acb7142db387443e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b7899fcdab4cfa9d6895ebceedc22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25eb2d6e212b4b669151ac8055885b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a6648456fa41acb7142db387443e6f",
            "placeholder": "​",
            "style": "IPY_MODEL_9ef52bde73ac4820bb81182c618c0dbd",
            "value": " 87%"
          }
        },
        "3130bb47ab58470d9a888ec11553b035": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5173742c3b2a4337940e66c7eb303157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbbf068fae424a2c8f5655884d7d885d",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23b7899fcdab4cfa9d6895ebceedc22e",
            "value": 8656
          }
        },
        "527702ac599a45f181663eea8ad78e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25eb2d6e212b4b669151ac8055885b84",
              "IPY_MODEL_5173742c3b2a4337940e66c7eb303157",
              "IPY_MODEL_685953149a91470ab126d945bfa054a6"
            ],
            "layout": "IPY_MODEL_7f7ba58234a04495a007d0daf8c88fa8"
          }
        },
        "685953149a91470ab126d945bfa054a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3130bb47ab58470d9a888ec11553b035",
            "placeholder": "​",
            "style": "IPY_MODEL_6b814c4023f843de8948d77438f1d693",
            "value": " 8656/10000 [08:10&lt;01:25, 15.66it/s]"
          }
        },
        "6b814c4023f843de8948d77438f1d693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7ba58234a04495a007d0daf8c88fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef52bde73ac4820bb81182c618c0dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbbf068fae424a2c8f5655884d7d885d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
