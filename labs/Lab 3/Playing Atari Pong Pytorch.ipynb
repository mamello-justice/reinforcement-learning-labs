{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd22t268l6tX"
      },
      "source": [
        "# Reinforcement Learning (RL)\n",
        "\n",
        "## Playing Atari with Deep RL [[Paper](https://arxiv.org/abs/1312.5602)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7CzsZ9YrClJ2",
        "outputId": "cb8e9d6a-8596-4432-9b18-12592ba2d7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.26.1 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (0.26.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym==0.26.1) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym==0.26.1) (1.23.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym==0.26.1) (2.2.0)\n",
            "Requirement already satisfied: gym[atari] in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym[atari]) (1.23.3)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym[atari]) (2.2.0)\n",
            "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from gym[atari]) (0.8.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (0.4.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from autorom[accept-rom-license]) (4.64.1)\n",
            "Requirement already satisfied: click in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from autorom[accept-rom-license]) (8.1.3)\n",
            "Requirement already satisfied: requests in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from autorom[accept-rom-license]) (2.28.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from autorom[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from click->autorom[accept-rom-license]) (0.4.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (1.26.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym == 0.26.1\n",
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHgCzH2mJxP"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mPuZ-jKOmORc",
        "outputId": "2dcd1dc4-ea8c-462a-dcb2-654e9ee98aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gym==0.26.1\n",
            "numpy==1.23.3\n",
            "torch==1.12.1+cpu\n",
            "tensorboard==1.15.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.tensorboard as tb\n",
        "import tensorboard\n",
        "\n",
        "import gym\n",
        "from gym import spaces, wrappers\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import trange\n",
        "except Exception:\n",
        "    trange = range\n",
        "\n",
        "print(f\"gym=={gym.__version__}\")\n",
        "print(f\"numpy=={np.__version__}\")\n",
        "print(f\"torch=={torch.__version__}\")\n",
        "print(f\"tensorboard=={tensorboard.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Idqj9nKLniAk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Frmb37S_oN0j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6tzupwRzjCu"
      },
      "source": [
        "## Wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IkEUiiFv-uFI"
      },
      "outputs": [],
      "source": [
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(\n",
        "                1, self.noop_max + 1\n",
        "            )  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs, info = None, None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _, info = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs, info = self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _, info = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _, info = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _, info = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs, info\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros(\n",
        "            (2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        ob, _ = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, trunc, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, trunc, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpGXom6qK8p"
      },
      "source": [
        "## Environmnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t47LFudfqM2y"
      },
      "outputs": [],
      "source": [
        "def trigger(x):\n",
        "    if x % 100 == 0:\n",
        "        print(x)\n",
        "    return x > hyper_params['num_steps'] - 600\n",
        "\n",
        "\n",
        "class TrainingEnvironment:\n",
        "    def __init__(self, env_name, seed):       \n",
        "        assert \"NoFrameskip\" in env_name, \"Require environment with no frameskip\"\n",
        "\n",
        "        self.seed = seed\n",
        "\n",
        "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "        self.env.seed(seed)\n",
        "\n",
        "        self.env = NoopResetEnv(self.env, noop_max=30)\n",
        "        self.env = MaxAndSkipEnv(self.env, skip=4)\n",
        "        self.env = EpisodicLifeEnv(self.env)\n",
        "        self.env = FireResetEnv(self.env)\n",
        "        self.env = ClipRewardEnv(self.env)\n",
        "        self.env = WarpFrame(self.env)\n",
        "        self.env = PyTorchFrame(self.env)\n",
        "        self.env = wrappers.RecordVideo(self.env, \"videos\")\n",
        "\n",
        "    def unwrap(self):\n",
        "        return self.env\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"seed = {self.seed}\\nstates = {self.env.observation_space.shape}\\nactions = {self.env.action_space.n}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_9JYFzzoTI"
      },
      "source": [
        "## Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4IphrCLm-o5J"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, size, device):\n",
        "        self._maxsize = size\n",
        "        self._mem_counter = 0\n",
        "        self._state_shape = env.observation_space.shape\n",
        "        self._num_actions = env.action_space.n\n",
        "        self._device = device\n",
        "\n",
        "        self.states = torch.zeros(self._maxsize,\n",
        "                                  *self._state_shape,\n",
        "                                  dtype=torch.float32).to(self._device)\n",
        "        self.actions = torch.zeros(self._maxsize,\n",
        "                                   dtype=torch.int64).to(self._device)\n",
        "        self.rewards = torch.zeros(self._maxsize,\n",
        "                                   dtype=torch.float32).to(self._device)\n",
        "        self.next_states = torch.zeros(self._maxsize,\n",
        "                                       *self._state_shape,\n",
        "                                       dtype=torch.float32).to(self._device)\n",
        "        self.dones = torch.zeros(self._maxsize,\n",
        "                                 dtype=torch.float32).to(self._device)\n",
        "\n",
        "    def __call__(self, indices):\n",
        "        return self.states[indices],\\\n",
        "            self.actions[indices],\\\n",
        "            self.rewards[indices],\\\n",
        "            self.next_states[indices],\\\n",
        "            self.dones[indices]\n",
        "\n",
        "    def state_dict(self):\n",
        "        dict = {\n",
        "            'maxsize': self._maxsize,\n",
        "            'mem_counter': self._mem_counter,\n",
        "            'state_shape': self._state_shape,\n",
        "            'num_actions': self._num_actions,\n",
        "            'states': self.states.detach(),\n",
        "            'actions': self.actions.detach(),\n",
        "            'rewards': self.rewards.detach(),\n",
        "            'next_state': self.next_states.detach(),\n",
        "            'dones': self.dones.detach(),\n",
        "        }\n",
        "\n",
        "        return dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._maxsize = state_dict['maxsize']\n",
        "        self._mem_counter = state_dict['mem_counter']\n",
        "        self._state_shape = state_dict['state_shape']\n",
        "        self._num_actions = state_dict['num_actions']\n",
        "\n",
        "        self.states = torch.load(state_dict['states'],\n",
        "                                 map_location=self._device)\n",
        "        self.actions = torch.load(state_dict['actions'],\n",
        "                                  map_location=self._device)\n",
        "        self.rewards = torch.load(state_dict['rewards'],\n",
        "                                  map_location=self._device)\n",
        "        self.next_states = torch.load(state_dict['next_states'],\n",
        "                                      map_location=self._device)\n",
        "        self.dones = torch.load(state_dict['dones'],\n",
        "                                map_location=self._device)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        next_index = self._mem_counter % self._maxsize\n",
        "\n",
        "        self.states[next_index] = torch.from_numpy(state)\n",
        "        self.actions[next_index] = action\n",
        "        self.rewards[next_index] = reward\n",
        "        self.next_states[next_index] = torch.from_numpy(next_state)\n",
        "        self.dones[next_index] = float(done)\n",
        "\n",
        "        self._mem_counter += 1\n",
        "\n",
        "    def last_frames(self, n):\n",
        "        \"\"\"\n",
        "        Get last n frames in the memory buffer\n",
        "        :param n: the number of frames\n",
        "        :return: a batch of n frames\n",
        "        \"\"\"\n",
        "\n",
        "        memory = min(self._mem_counter, self._maxsize) - 1\n",
        "        assert self._mem_counter >= n, f\"memory={memory} must have at least n={n} frames\"\n",
        "\n",
        "        index = self._mem_counter % self._maxsize\n",
        "        indices = range(index-n, index)\n",
        "\n",
        "        return self(indices)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        memory = min(self._mem_counter, self._maxsize) - 1\n",
        "        assert self._mem_counter >= batch_size, f\"memory={memory} must have at least batch_size={batch_size} frames\"\n",
        "\n",
        "        indices = np.random.randint(0, memory, size=batch_size)\n",
        "\n",
        "        return self(indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dB0U6g8z0Fv"
      },
      "source": [
        "## Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-justdcy-c1R"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, env, memory_size, use_double_dqn, lr, batch_size, gamma, device, log_dir):\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.device = device\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        self.episode_rewards = [0.0]\n",
        "\n",
        "        # Tensorboard\n",
        "        self.tb_w = tb.SummaryWriter(log_dir)\n",
        "\n",
        "        # Q-networks\n",
        "        self.Q = self._build_model().to(self.device)\n",
        "        self.Q_target = self._build_model().to(self.device)\n",
        "\n",
        "        # Loss\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "        # Adam optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=lr)\n",
        "\n",
        "        # Replay buffer/memory\n",
        "        self.memory = ReplayBuffer(env=env, size=memory_size, device=device)\n",
        "\n",
        "        # Indexing\n",
        "        self.idx = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        # 1 channel -> 16 features -> 32 features -> 32*9*9=2592 features -> 256 -> # actions\n",
        "        return nn.Sequential(nn.Conv2d(1, 16, kernel_size=8, stride=4, padding=0),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Conv2d(16, 32, kernel_size=4,\n",
        "                                       stride=2, padding=0),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Flatten(),\n",
        "                             nn.Linear(32*9*9, 256), nn.ReLU(),\n",
        "                             nn.Linear(256, self.num_actions))\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = \\\n",
        "            self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Normalize memory states\n",
        "        states = states / 255\n",
        "        next_states = next_states / 255\n",
        "\n",
        "        # Targets\n",
        "        with torch.no_grad():  # Not used in gradient calculation\n",
        "            if self.use_double_dqn:\n",
        "                target_actions = target_values = self.Q(next_states)\\\n",
        "                    .argmax(dim=1, keepdim=True)\n",
        "\n",
        "                target_values = self.Q_target(next_states)\\\n",
        "                    .gather(dim=1, index=target_actions).flatten()\n",
        "\n",
        "            else:\n",
        "                # torch.max() -> Tensor[values], Tensor[indices]\n",
        "                target_values = self.Q_target(next_states)\\\n",
        "                    .max(dim=1, keepdim=True)[0].flatten()\n",
        "\n",
        "            targets = rewards + \\\n",
        "                self.gamma * target_values * (1 - dones)\n",
        "\n",
        "        # Online\n",
        "        values = self.Q(states)\\\n",
        "            .gather(dim=1, index=actions.unsqueeze(-1))\\\n",
        "            .flatten()\n",
        "\n",
        "        # Loss\n",
        "        loss = self.loss(values, targets).to(self.device)\n",
        "\n",
        "        # Gradient descent\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Log to tensorboard\n",
        "        self.tb_w.add_scalar(\"Loss\", loss.item(), self.idx)\n",
        "        for name, weight in self.Q.named_parameters():\n",
        "            self.tb_w.add_histogram('Q-%s' % name, weight, self.idx)\n",
        "            self.tb_w.add_histogram('Q-%s.grad' % name, weight, self.idx)\n",
        "\n",
        "        for name, weight in self.Q_target.named_parameters():\n",
        "            self.tb_w.add_histogram('Q-target-%s' % name, weight, self.idx)\n",
        "            self.tb_w.add_histogram('Q-target-%s' % name, weight, self.idx)\n",
        "\n",
        "        self.idx += 1\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Rewards\n",
        "        self.episode_rewards[-1] += reward\n",
        "\n",
        "        if done:\n",
        "            self.episode_rewards.append(0.0)\n",
        "\n",
        "    def num_episodes(self):\n",
        "        return len(self.episode_rewards)\n",
        "\n",
        "    def act(self, state: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # Last four frames (states)\n",
        "        batch, _, _, _, _ = self.memory.last_frames(4)\n",
        "\n",
        "        # Replace first frame (state)\n",
        "        batch[0, :, :, :] = state\n",
        "\n",
        "        # Normalize memory states\n",
        "        batch = batch / 255\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Greedy action (max)\n",
        "            return self.Q(batch).argmax(dim=1, keepdim=True)[1].view(1, 1)\n",
        "\n",
        "    def log(self, t):\n",
        "        self.tb_w.add_scalar(\"Episodes\", self.num_episodes(), t)\n",
        "\n",
        "        mean_100ep_reward = round(np.mean(self.episode_rewards[-101:-1]), 1)\n",
        "        self.tb_w.add_scalar(\"Mean Reward (100 episodes)\",\n",
        "                             mean_100ep_reward, t)\n",
        "\n",
        "    def save(self, path):\n",
        "        print('saving models...')\n",
        "\n",
        "        checkpoint = {\n",
        "            'Q': self.Q.state_dict(),\n",
        "            'Q_target': self.Q_target.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'loss': self.loss,\n",
        "            'memory': self.memory.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        print('loading models...')\n",
        "\n",
        "        checkpoint = torch.load(path)\n",
        "\n",
        "        self.Q.load_state_dict(checkpoint.get('Q'), map_location=self.device)\n",
        "        self.Q.train()\n",
        "\n",
        "        self.Q_target.load_state_dict(checkpoint.get(\n",
        "            'Q_target'), map_location=self.device)\n",
        "        self.Q_target.train()\n",
        "\n",
        "        self.optimizer.load_state_dict(checkpoint.get('optimizer'))\n",
        "\n",
        "        self.loss = checkpoint.get('loss')\n",
        "\n",
        "        self.memory.load_state_dict(checkpoint.get('memory'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyper_params = {\n",
        "    \"seed\": 42,\n",
        "    \"env\": \"PongNoFrameskip-v4\",\n",
        "    \"use-double-dqn\": True,\n",
        "    \"replay-buffer-size\": int(5e3),\n",
        "    \"batch-size\": 256,\n",
        "    \"num-steps\": int(1e6),\n",
        "    \"learning-starts\": int(1e4),\n",
        "    \"learning-freq\": 5,\n",
        "    \"target-update-freq\": int(1e3),\n",
        "    \"learning-rate\": int(1e-4),\n",
        "    \"discount-factor\": 0.99,\n",
        "    \"eps-start\": 1.0,\n",
        "    \"eps-end\": 0.01,\n",
        "    \"eps-fraction\": 0.1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqnp_A4HshEi"
      },
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "527702ac599a45f181663eea8ad78e86",
            "25eb2d6e212b4b669151ac8055885b84",
            "5173742c3b2a4337940e66c7eb303157",
            "685953149a91470ab126d945bfa054a6",
            "7f7ba58234a04495a007d0daf8c88fa8",
            "21a6648456fa41acb7142db387443e6f",
            "9ef52bde73ac4820bb81182c618c0dbd",
            "fbbf068fae424a2c8f5655884d7d885d",
            "23b7899fcdab4cfa9d6895ebceedc22e",
            "3130bb47ab58470d9a888ec11553b035",
            "6b814c4023f843de8948d77438f1d693"
          ]
        },
        "id": "FhZGtULnsjSA",
        "outputId": "f4dfae22-a816-4994-f23a-7a6b9617e635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LENOVO\\.pyenv-win-venv\\envs\\rl_lab_3\\lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\LENOVO\\Develop\\coms4061a-labs\\labs\\Lab 3\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seed = 42\n",
            "states = (1, 84, 84)\n",
            "actions = 6\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "DQNAgent.__init__() missing 1 required positional argument: 'tb_path'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(env_wrapper)\n\u001b[0;32m     20\u001b[0m memory \u001b[38;5;241m=\u001b[39m ReplayBuffer(hyper_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 22\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     23\u001b[0m                  memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[0;32m     24\u001b[0m                  use_double_dqn\u001b[38;5;241m=\u001b[39mhyper_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_dqn\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     25\u001b[0m                  lr\u001b[38;5;241m=\u001b[39mhyper_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearn_rate\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     26\u001b[0m                  batch_size\u001b[38;5;241m=\u001b[39mhyper_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     27\u001b[0m                  gamma\u001b[38;5;241m=\u001b[39mhyper_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscount\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     28\u001b[0m                  device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     30\u001b[0m stats \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisodes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean reward (100 eps)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplore time\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     31\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m]\n",
            "\u001b[1;31mTypeError\u001b[0m: DQNAgent.__init__() missing 1 required positional argument: 'tb_path'"
          ]
        }
      ],
      "source": [
        "args_vars = {\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'log_dir': 'runs',\n",
        "    'stat_freq': 20,\n",
        "    'out': 'data/model.pth'\n",
        "}\n",
        "\n",
        "def get_epsilon_threshold(t):\n",
        "    diff = hyper_params['eps-end'] - hyper_params['eps-start']\n",
        "    fraction = float(t) / hyper_params['eps-fraction'] * \\\n",
        "        float(hyper_params['num-steps'])\n",
        "    fraction = min(1.0, fraction)\n",
        "    return hyper_params['eps-start'] + fraction * diff\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(args_vars.get(\"device\"))\n",
        "    print(\"device = %s\" % device)\n",
        "\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "\n",
        "    train_env = TrainingEnvironment(env_name=hyper_params['env'],\n",
        "                                    seed=hyper_params['seed'],\n",
        "                                    steps=hyper_params['num-steps'])\n",
        "\n",
        "    env = train_env.unwrap()\n",
        "\n",
        "    agent = DQNAgent(env=env,\n",
        "                        memory_size=hyper_params['replay-buffer-size'],\n",
        "                        use_double_dqn=hyper_params['use-double-dqn'],\n",
        "                        lr=hyper_params['learning-rate'],\n",
        "                        batch_size=hyper_params['batch-size'],\n",
        "                        gamma=hyper_params['discount-factor'],\n",
        "                        device=device,\n",
        "                        log_dir=args_vars['log_dir'])\n",
        "\n",
        "    in_file = args_vars.get('in')\n",
        "    if in_file is not None:\n",
        "        try:\n",
        "            agent.load(in_file)\n",
        "        except FileNotFoundError:\n",
        "            print(\"model file not found: %s\" % in_file)\n",
        "\n",
        "    try:\n",
        "        state, _ = env.reset()\n",
        "        for t in trange(hyper_params['num-steps']):\n",
        "            epsilon_threshold = get_epsilon_threshold(t, hyper_params)\n",
        "\n",
        "            #  select random action if sample is less equal than eps_threshold\n",
        "            if (t <= hyper_params['learning-starts'] or random.random() <= epsilon_threshold):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = agent.act(torch.tensor(state))\n",
        "\n",
        "            # take step in env\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "            agent.remember(state=state,\n",
        "                            action=action,\n",
        "                            reward=reward,\n",
        "                            next_state=next_state,\n",
        "                            done=float(done))\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                state, _ = env.reset()\n",
        "\n",
        "                if (agent.num_episodes() % args_vars.get('stat_freq') == 0):\n",
        "                    explore_time = int(100 * epsilon_threshold)\n",
        "                    agent.log(t)\n",
        "                    agent.tb_w.add_scalar(\"Explore time\", explore_time, t)\n",
        "\n",
        "            if t > hyper_params['learning-starts']:\n",
        "                if t % hyper_params['learning-freq'] == 0:\n",
        "                    agent.optimise_td_loss()\n",
        "\n",
        "                if t % hyper_params['target-update-freq'] == 0:\n",
        "                    agent.update_target_network()\n",
        "\n",
        "    finally:\n",
        "        agent.tb_w.close()\n",
        "        out_file = args_vars.get('out')\n",
        "        if out_file is not None:\n",
        "            agent.save(out_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mWHgCzH2mJxP",
        "lF17Bj-8nfSk",
        "e6tzupwRzjCu",
        "rZpGXom6qK8p",
        "9WmQIaalzuev",
        "8dB0U6g8z0Fv",
        "Wqnp_A4HshEi"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "98273bd8a540ecd20beace9e7ace114571d037c3f5382e4595714f5a9fa1d3c0"
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 ('rl_lab_3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21a6648456fa41acb7142db387443e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b7899fcdab4cfa9d6895ebceedc22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25eb2d6e212b4b669151ac8055885b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a6648456fa41acb7142db387443e6f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9ef52bde73ac4820bb81182c618c0dbd",
            "value": " 87%"
          }
        },
        "3130bb47ab58470d9a888ec11553b035": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5173742c3b2a4337940e66c7eb303157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbbf068fae424a2c8f5655884d7d885d",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23b7899fcdab4cfa9d6895ebceedc22e",
            "value": 8656
          }
        },
        "527702ac599a45f181663eea8ad78e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25eb2d6e212b4b669151ac8055885b84",
              "IPY_MODEL_5173742c3b2a4337940e66c7eb303157",
              "IPY_MODEL_685953149a91470ab126d945bfa054a6"
            ],
            "layout": "IPY_MODEL_7f7ba58234a04495a007d0daf8c88fa8"
          }
        },
        "685953149a91470ab126d945bfa054a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3130bb47ab58470d9a888ec11553b035",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6b814c4023f843de8948d77438f1d693",
            "value": " 8656/10000 [08:10&lt;01:25, 15.66it/s]"
          }
        },
        "6b814c4023f843de8948d77438f1d693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7ba58234a04495a007d0daf8c88fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef52bde73ac4820bb81182c618c0dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbbf068fae424a2c8f5655884d7d885d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
