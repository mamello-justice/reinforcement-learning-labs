{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd22t268l6tX"
      },
      "source": [
        "# Reinforcement Learning (RL)\n",
        "\n",
        "## Human-level control through deep rl [[Paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)]\n",
        "## Playing Atari with Deep RL [[Paper](https://arxiv.org/abs/1312.5602)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7CzsZ9YrClJ2",
        "outputId": "cb8e9d6a-8596-4432-9b18-12592ba2d7f4"
      },
      "outputs": [],
      "source": [
        "%pip install gym==0.26.1 --quiet\n",
        "%pip install gym[atari] --quiet\n",
        "%pip install autorom[accept-rom-license] --quiet\n",
        "\n",
        "%pip install comet_ml --quiet\n",
        "%pip install tensorboardX --quiet\n",
        "%pip install onnx --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comet Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import comet_ml\n",
        "comet_ml.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHgCzH2mJxP"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mPuZ-jKOmORc",
        "outputId": "2dcd1dc4-ea8c-462a-dcb2-654e9ee98aff"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorboardX as tb\n",
        "\n",
        "import gym\n",
        "from gym import spaces, wrappers\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import trange\n",
        "except Exception:\n",
        "    trange = range\n",
        "\n",
        "print(f\"gym=={gym.__version__}\")\n",
        "print(f\"numpy=={np.__version__}\")\n",
        "print(f\"torch=={torch.__version__}\")\n",
        "print(f\"tensorboardX=={tb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6tzupwRzjCu"
      },
      "source": [
        "## Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkEUiiFv-uFI"
      },
      "outputs": [],
      "source": [
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(\n",
        "                1, self.noop_max + 1\n",
        "            )  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs, info = None, None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _, info = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs, info = self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _, info = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _, info = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _, info = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs, info\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros(\n",
        "            (2, *env.observation_space.shape), dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        ob, info = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, trunc, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, trunc, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpGXom6qK8p"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t47LFudfqM2y"
      },
      "outputs": [],
      "source": [
        "class TrainingEnvironment:\n",
        "    def __init__(self, env_name, seed, steps):       \n",
        "        assert \"NoFrameskip\" in env_name, \"Require environment with no frameskip\"\n",
        "\n",
        "        self.seed = seed\n",
        "        self.steps = steps\n",
        "\n",
        "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "        self.env.seed(seed)\n",
        "\n",
        "        # Investigate... rgb_array is not detected\n",
        "        self.env = wrappers.RecordVideo(self.env, \"videos\", step_trigger=self.trigger)\n",
        "        self.env = NoopResetEnv(self.env, noop_max=30)\n",
        "        self.env = MaxAndSkipEnv(self.env, skip=4)\n",
        "        self.env = EpisodicLifeEnv(self.env)\n",
        "        self.env = FireResetEnv(self.env)\n",
        "        self.env = ClipRewardEnv(self.env)\n",
        "        self.env = WarpFrame(self.env)\n",
        "        self.env = PyTorchFrame(self.env)\n",
        "        self.env = FrameStack(self.env, k=4)\n",
        "\n",
        "    def unwrap(self):\n",
        "        return self.env\n",
        "    \n",
        "    def trigger(self, x):\n",
        "        return x > self.steps - 600\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"seed = {self.seed}\\nstates = {self.env.observation_space.shape}\\nactions = {self.env.action_space.n}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_9JYFzzoTI"
      },
      "source": [
        "## Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IphrCLm-o5J"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  N, observation_shape, num_actions):\n",
        "        self.N = N\n",
        "        self._mem_counter = 0\n",
        "        self._observation_shape = observation_shape\n",
        "        self._num_actions = num_actions\n",
        "\n",
        "        self.states = np.zeros((self.N, *self._observation_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(self.N, dtype=np.int64)\n",
        "        self.rewards = np.zeros(self.N, dtype=np.float32)\n",
        "        self.next_states = np.zeros((self.N, *self._observation_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros(self.N, dtype=np.float32)\n",
        "\n",
        "    def __call__(self, indices):\n",
        "        return self.states[indices],\\\n",
        "            self.actions[indices],\\\n",
        "            self.rewards[indices],\\\n",
        "            self.next_states[indices],\\\n",
        "            self.dones[indices]\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        next_index = self._mem_counter % self.N\n",
        "\n",
        "        self.states[next_index] = np.array(state)\n",
        "        self.actions[next_index] = action\n",
        "        self.rewards[next_index] = reward\n",
        "        self.next_states[next_index] = np.array(next_state)\n",
        "        self.dones[next_index] = float(done)\n",
        "\n",
        "        self._mem_counter += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        memory = min(self._mem_counter, self.N) - 1\n",
        "        assert self._mem_counter >= batch_size, f\"memory={memory} must have at least batch_size={batch_size} frames\"\n",
        "\n",
        "        indices = np.random.randint(0, memory, size=batch_size)\n",
        "\n",
        "        return self(indices)\n",
        "\n",
        "    def state_dict(self):\n",
        "        dict = {\n",
        "            'N': self.N,\n",
        "            'mem_counter': self._mem_counter,\n",
        "            'observation_shape': self._observation_shape,\n",
        "            'num_actions': self._num_actions,\n",
        "            'states': self.states,\n",
        "            'actions': self.actions,\n",
        "            'rewards': self.rewards,\n",
        "            'next_state': self.next_states,\n",
        "            'dones': self.dones,\n",
        "        }\n",
        "\n",
        "        return dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.N = state_dict['N']\n",
        "        self._mem_counter = state_dict['mem_counter']\n",
        "        self._observation_shape = state_dict['observation_shape']\n",
        "        self._num_actions = state_dict['num_actions']\n",
        "\n",
        "        self.states = np.array(state_dict['states'], dtype=np.float32)\n",
        "        self.actions = np.array(state_dict['actions'], dtype=np.int64)\n",
        "        self.rewards = np.array(state_dict['rewards'], dtype=np.float32)\n",
        "        self.next_states = np.array(state_dict['next_states'], dtype=np.float32)\n",
        "        self.dones = np.array(state_dict['dones'], dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_actions):\n",
        "        super(DQNModel, self).__init__()\n",
        "\n",
        "        # no. channels -> 32 features -> 64 features -> 64*7*7=3136 features -> 512 -> no. actions\n",
        "        self.model = nn.Sequential(\n",
        "            # First hidden layer\n",
        "            nn.Conv2d(num_input_channels, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Second hidden layer\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Third hidden layer\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Convolution to linear\n",
        "            nn.Flatten(),\n",
        "            \n",
        "            # Final hidden layer\n",
        "            nn.Linear(64*7*7, 512),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Output layer\n",
        "            nn.Linear(512, num_actions))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dB0U6g8z0Fv"
      },
      "source": [
        "## DQN Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-justdcy-c1R"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, env, memory_size, use_double_dqn, lr, batch_size, gamma, device, log_dir, comet_config, log_hists):\n",
        "        self.observation_shape = env.observation_space.shape\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.device = device\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        self.episode_rewards = [0.0]\n",
        "        self.should_log_hists = log_hists\n",
        "\n",
        "        # Tensorboard + Comet\n",
        "        self.tb_w = tb.SummaryWriter(log_dir=log_dir, comet_config=comet_config)\n",
        "\n",
        "        # Initialize replay memory D to capacity N\n",
        "        self.D = ReplayBuffer(N=memory_size,\n",
        "                              observation_shape=self.observation_shape,\n",
        "                              num_actions=self.num_actions)\n",
        "\n",
        "        # Initialize action-value function Q with random weights θ\n",
        "        self.Q = DQNModel(self.observation_shape[0], self.num_actions).to(self.device)\n",
        "        \n",
        "        # Initialize target action-value function Q^ with weights θ_ = θ\n",
        "        self.Q_ = DQNModel(self.observation_shape[0], self.num_actions).to(self.device)\n",
        "        self.update_target_network()\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Adam optimizer for Q network\n",
        "        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=lr)\n",
        "\n",
        "        # Indexing\n",
        "        self.num_optims = 0\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # NB: Unsqueezed a layer for unit batch\n",
        "        state = torch.tensor(np.array([state]), dtype=torch.float32, device=self.device) / 255\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # select a_t = argmax a Q(s,a|θ)\n",
        "            return self.Q(state).argmax(dim=1, keepdim=True).item()\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        # Sample random minibatch of transitions (state, action, reward, next_state, done) from D\n",
        "        batch = self.D.sample(self.batch_size)\n",
        "        \n",
        "        # Alternative to .gather()\n",
        "        indices = np.arange(self.batch_size)\n",
        "        \n",
        "        next_states = torch.tensor(batch[3], device=self.device) / 255\n",
        "\n",
        "        with torch.no_grad():  # Not used in gradient calculation\n",
        "            if self.use_double_dqn:\n",
        "                # a' = argmax a Q(s+1,a|θ)\n",
        "                actions_ = self.Q.forward(next_states)\\\n",
        "                    .argmax(dim=1, keepdim=True)\\\n",
        "                    .squeeze()\n",
        "                    \n",
        "\n",
        "                # Q^(s+1,a'|θ_)\n",
        "                max_values = self.Q_.forward(next_states)[indices, actions_]\n",
        "\n",
        "            else:\n",
        "                # max a' Q^(s+1,a'|θ_)\n",
        "                # NB: torch.max() returns tuple(Tensor[values], Tensor[indices])\n",
        "                max_values = self.Q_.forward(next_states)\\\n",
        "                    .max(dim=1, keepdim=True)[0]\\\n",
        "                    .squeeze()\n",
        "\n",
        "            rewards = torch.tensor(batch[2], device=self.device)\n",
        "            dones = torch.tensor(batch[4], device=self.device)\n",
        "            \n",
        "            # Set y_j\n",
        "            # NB: * (1 - dones) handles episodes terminating at step j+1 (will be 0 and only rewards considered)\n",
        "            Y = rewards + self.gamma * max_values * (1 - dones)\n",
        "\n",
        "        states = torch.tensor(batch[0], device=self.device) / 255\n",
        "        actions = torch.tensor(batch[1], device=self.device)\n",
        "        \n",
        "        # Q(s,a|θ)\n",
        "        values = self.Q.forward(states)[indices, actions]\n",
        "\n",
        "        # Perform a gradient descent step on (Y - Q(s,a|θ))^2 with respect to the network parameters θ\n",
        "        loss = self.criterion(Y, values).to(self.device)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Log to tensorboard\n",
        "        self.tb_w.add_scalar(\"data/loss\", loss.item(), self.num_optims)\n",
        "\n",
        "        if self.should_log_hists:\n",
        "            # Expensive comp\n",
        "            self.log_hists()\n",
        "\n",
        "        self.num_optims += 1\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        self.Q_.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # Store transition (state, action, reward, next_state, done) in D\n",
        "        self.D.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Rewards\n",
        "        self.episode_rewards[-1] += reward\n",
        "\n",
        "        if done:\n",
        "            self.episode_rewards.append(0.0)\n",
        "\n",
        "    def num_episodes(self):\n",
        "        return len(self.episode_rewards)    \n",
        "\n",
        "\n",
        "    # Logging\n",
        "    def log_hists(self):\n",
        "        for name, weight in self.Q.named_parameters():\n",
        "            self.tb_w.add_histogram('weights/Q-%s' % name, weight, self.num_optims)\n",
        "            self.tb_w.add_histogram('weights/Q-%s.grad' % name, weight, self.num_optims)\n",
        "\n",
        "        for name, weight in self.Q_.named_parameters():\n",
        "            self.tb_w.add_histogram('weights/Q-target-%s' % name, weight, self.num_optims)\n",
        "            self.tb_w.add_histogram('weights/Q-target-%s' % name, weight, self.num_optims)\n",
        "\n",
        "    def log(self, t):\n",
        "        self.tb_w.add_scalar(\"data/episodes\", self.num_episodes(), t)\n",
        "\n",
        "        mean_100ep_reward = round(np.mean(self.episode_rewards[-101:-1]), 1)\n",
        "        self.tb_w.add_scalar(\"data/rewards\", mean_100ep_reward, t)\n",
        "\n",
        "\n",
        "    # Saving & Loading\n",
        "    def save(self, path):\n",
        "        print('saving checkpoint...')\n",
        "\n",
        "        checkpoint = {\n",
        "            'Q': self.Q.state_dict(),\n",
        "            'Q_': self.Q_.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'loss': self.criterion,\n",
        "            'D': self.D.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        print('loading checkpoints...')\n",
        "\n",
        "        checkpoint = torch.load(path)\n",
        "\n",
        "        self.Q.load_state_dict(checkpoint.get('Q'), map_location=self.device)\n",
        "        self.Q.train()\n",
        "\n",
        "        self.Q_.load_state_dict(checkpoint.get('Q_'),\n",
        "                                map_location=self.device)\n",
        "\n",
        "        self.optimizer.load_state_dict(checkpoint.get('optimizer'))\n",
        "\n",
        "        self.criterion = checkpoint.get('loss')\n",
        "\n",
        "        self.D.load_state_dict(checkpoint.get('D'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args_vars = {\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'log_hists': False,\n",
        "    'log_dir': None,\n",
        "    'stat_freq': 2,\n",
        "    'out': 'data/model.pth',\n",
        "    'checkpoint_freq': int(1e4),\n",
        "    'comet_config': { 'disabled': False }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyper_params = {\n",
        "    \"seed\": 1,\n",
        "    \"env\": \"PongNoFrameskip-v4\",\n",
        "    \"use-double-dqn\": False,\n",
        "    \"replay-buffer-size\": int(5e3),\n",
        "    \"batch-size\": 32,\n",
        "    \"num-steps\": int(1e6),\n",
        "    \"learning-starts\": int(5e4),\n",
        "    \"learning-freq\": 4,\n",
        "    \"target-update-freq\": int(1e4),\n",
        "    \"learning-rate\": int(1e-4),\n",
        "    \"discount-factor\": 0.99,\n",
        "    \"eps-start\": 1.0,\n",
        "    \"eps-end\": 0.1,\n",
        "    \"eps-fraction\": 0.1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_onnx(agent, state):\n",
        "        state = torch.tensor(np.array([state]), dtype=torch.float32, device=agent.device) / 255        \n",
        "        filename = 'data/model.onnx'\n",
        "        input_axis = { 0: \"batch_size\", 1: \"channel\", 2: \"height\", 3: \"width\"}\n",
        "        output_axis = { 0: \"action\" }\n",
        "        \n",
        "        torch.onnx.export(agent.Q, \n",
        "                          (state,), \n",
        "                          filename,\n",
        "                          export_params=True,\n",
        "                          opset_version=10,\n",
        "                          do_constant_folding=True,\n",
        "                          input_names=[\"input\"],\n",
        "                          output_names=[\"output\"],\n",
        "                          dynamic_axes={ \"input\": input_axis, \"output\": output_axis })\n",
        "        \n",
        "        agent.tb_w.add_onnx_graph(filename)\n",
        "\n",
        "def get_epsilon_threshold(t, hyper_params):\n",
        "    eps_end = hyper_params['eps-end']\n",
        "    eps_start = hyper_params['eps-start']\n",
        "    eps_fraction = hyper_params['eps-fraction']\n",
        "    steps = hyper_params['num-steps']\n",
        "    \n",
        "    fraction = min(1.0, float(t) / (eps_fraction * float(steps)))\n",
        "    return eps_start + fraction * (eps_end - eps_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqnp_A4HshEi"
      },
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "527702ac599a45f181663eea8ad78e86",
            "25eb2d6e212b4b669151ac8055885b84",
            "5173742c3b2a4337940e66c7eb303157",
            "685953149a91470ab126d945bfa054a6",
            "7f7ba58234a04495a007d0daf8c88fa8",
            "21a6648456fa41acb7142db387443e6f",
            "9ef52bde73ac4820bb81182c618c0dbd",
            "fbbf068fae424a2c8f5655884d7d885d",
            "23b7899fcdab4cfa9d6895ebceedc22e",
            "3130bb47ab58470d9a888ec11553b035",
            "6b814c4023f843de8948d77438f1d693"
          ]
        },
        "id": "FhZGtULnsjSA",
        "outputId": "f4dfae22-a816-4994-f23a-7a6b9617e635"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # default_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # args_vars = setup_args(default_device=default_device)\n",
        "    # hyper_params = args_to_hyper_params(args_vars)\n",
        "    \n",
        "    device = torch.device(args_vars.get(\"device\"))\n",
        "    print(\"device = %s\" % device)\n",
        "\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "\n",
        "    train_env = TrainingEnvironment(env_name=hyper_params['env'],\n",
        "                                    seed=hyper_params['seed'],\n",
        "                                    steps=hyper_params['num-steps'])\n",
        "\n",
        "    env = train_env.unwrap()\n",
        "    \n",
        "\n",
        "    agent = DQNAgent(env=env, \n",
        "                     memory_size=hyper_params['replay-buffer-size'], \n",
        "                     use_double_dqn=hyper_params['use-double-dqn'],\n",
        "                     lr=hyper_params['learning-rate'],\n",
        "                     batch_size=hyper_params['batch-size'],\n",
        "                     gamma=hyper_params['discount-factor'],\n",
        "                     device=device,\n",
        "                     log_dir=args_vars['log_dir'],\n",
        "                     comet_config=args_vars['comet_config'],\n",
        "                     log_hists=args_vars['log_hists'])\n",
        "\n",
        "\n",
        "    in_file = args_vars.get('in')\n",
        "    if in_file is not None:\n",
        "        try:\n",
        "            agent.load(in_file)\n",
        "        except FileNotFoundError:\n",
        "            print(\"model file not found: %s\" % in_file)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    agent.tb_w.add_hparams(hparam_dict=hyper_params, metric_dict={})\n",
        "    log_onnx(agent, state)\n",
        "    \n",
        "    out_file = args_vars.get('out')\n",
        "    checkpoint_freq = args_vars['checkpoint_freq']\n",
        "    try:\n",
        "        for t in trange(hyper_params['num-steps']):\n",
        "            epsilon_threshold = get_epsilon_threshold(t, hyper_params)\n",
        "\n",
        "            #  select random action if sample is less equal than eps_threshold\n",
        "            if (t <= hyper_params['learning-starts'] or random.random() <= epsilon_threshold):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = agent.act(state)\n",
        "\n",
        "            # take step in env\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "            agent.remember(state=state,\n",
        "                            action=action,\n",
        "                            reward=reward,\n",
        "                            next_state=next_state,\n",
        "                            done=float(done))\n",
        "\n",
        "            # update state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                state, _ = env.reset()\n",
        "                \n",
        "                if (agent.num_episodes() % args_vars.get('stat_freq') == 0):\n",
        "                    explore_time = int(100 * epsilon_threshold)\n",
        "                    agent.log(t)\n",
        "                    agent.tb_w.add_scalar(\"data/explore_time\", explore_time, t)\n",
        "\n",
        "            if t > hyper_params['learning-starts']:\n",
        "                if t % hyper_params['learning-freq'] == 0:\n",
        "                    agent.optimise_td_loss()\n",
        "\n",
        "                # every C steps reset Q^ = Q\n",
        "                if t % hyper_params['target-update-freq'] == 0:\n",
        "                    agent.update_target_network()\n",
        "                    \n",
        "            if (out_file is not None and checkpoint_freq is not None and t % checkpoint_freq == 0):\n",
        "                agent.save(out_file)\n",
        "                    \n",
        "        print(\"total episodes: \", agent.num_episodes())\n",
        "\n",
        "    finally:\n",
        "        agent.tb_w.close()\n",
        "        if out_file is not None:\n",
        "            agent.save(out_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mWHgCzH2mJxP",
        "lF17Bj-8nfSk",
        "e6tzupwRzjCu",
        "rZpGXom6qK8p",
        "9WmQIaalzuev",
        "8dB0U6g8z0Fv",
        "Wqnp_A4HshEi"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "98273bd8a540ecd20beace9e7ace114571d037c3f5382e4595714f5a9fa1d3c0"
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 ('rl_lab_3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21a6648456fa41acb7142db387443e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b7899fcdab4cfa9d6895ebceedc22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25eb2d6e212b4b669151ac8055885b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a6648456fa41acb7142db387443e6f",
            "placeholder": "​",
            "style": "IPY_MODEL_9ef52bde73ac4820bb81182c618c0dbd",
            "value": " 87%"
          }
        },
        "3130bb47ab58470d9a888ec11553b035": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5173742c3b2a4337940e66c7eb303157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbbf068fae424a2c8f5655884d7d885d",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23b7899fcdab4cfa9d6895ebceedc22e",
            "value": 8656
          }
        },
        "527702ac599a45f181663eea8ad78e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25eb2d6e212b4b669151ac8055885b84",
              "IPY_MODEL_5173742c3b2a4337940e66c7eb303157",
              "IPY_MODEL_685953149a91470ab126d945bfa054a6"
            ],
            "layout": "IPY_MODEL_7f7ba58234a04495a007d0daf8c88fa8"
          }
        },
        "685953149a91470ab126d945bfa054a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3130bb47ab58470d9a888ec11553b035",
            "placeholder": "​",
            "style": "IPY_MODEL_6b814c4023f843de8948d77438f1d693",
            "value": " 8656/10000 [08:10&lt;01:25, 15.66it/s]"
          }
        },
        "6b814c4023f843de8948d77438f1d693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7ba58234a04495a007d0daf8c88fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef52bde73ac4820bb81182c618c0dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbbf068fae424a2c8f5655884d7d885d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
