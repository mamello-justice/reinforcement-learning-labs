{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0582930-26c4-4bda-bb69-e7f61a075e7c",
   "metadata": {},
   "source": [
    "# COMS4061A - Reinforcement Learning\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "- Mamello Seboholi [1851317]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b75c1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c0ce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5bf8b",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfb7416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Position = tuple[int, int]\n",
    "Rewards = list[list[int]]\n",
    "\n",
    "def get_value(array, x, y, default = None):\n",
    "    if x < 0 or y < 0:\n",
    "        return default\n",
    "\n",
    "    if x >= len(array):\n",
    "        return default\n",
    "\n",
    "    row = array[x]\n",
    "\n",
    "    if y >= len(array):\n",
    "        return default\n",
    "\n",
    "    return row[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef8e38",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88dbc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, x_modifier, y_modifier, desc = \"Action\"):\n",
    "        self.x_modifier = x_modifier\n",
    "        self.y_modifier = y_modifier\n",
    "        self.desc = desc\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.desc}\"\n",
    "\n",
    "    def get_new_position(self, x: int, y: int) -> Position:\n",
    "        return self.x_modifier(x), self.y_modifier(y)\n",
    "\n",
    "UP = Action(lambda x: x, lambda y: y-1 , desc=\"UP\")\n",
    "RIGHT = Action(lambda x: x+1, lambda y: y, desc=\"RIGHT\")\n",
    "DOWN = Action(lambda x: x, lambda y: y+1, desc=\"DOWN\")\n",
    "LEFT = Action(lambda x: x-1, lambda y: y, desc=\"LEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1acc84",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e83548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, x, y, actions):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.actions = actions\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"[{self.x}][{self.y}] actions: {[str(action) for action in self.actions]}\"\n",
    "    \n",
    "States = list[list[State]]\n",
    "    \n",
    "def get_states_from_world(world) -> States:\n",
    "    states = []\n",
    "    for i, row in enumerate(world):\n",
    "        states_row = []\n",
    "        for j, cell in enumerate(row):\n",
    "            actions = []\n",
    "            # Up\n",
    "            if (get_value(world, i-1, j, -1) != -1):\n",
    "                actions.append(UP)\n",
    "\n",
    "            # Right\n",
    "            if (get_value(world, i, j+1, -1) != -1):\n",
    "                actions.append(RIGHT)\n",
    "\n",
    "            # Down\n",
    "            if (get_value(world, i+1, j, -1) != -1):\n",
    "                actions.append(DOWN)\n",
    "\n",
    "            # Left\n",
    "            if (get_value(world, i, j-1, -1) != -1):\n",
    "                actions.append(LEFT)\n",
    "\n",
    "            states_row.append(State(j, i, actions))\n",
    "        states.append(states_row)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74ee6b",
   "metadata": {},
   "source": [
    "### MPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39b5baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, states: States, rewards: Rewards):\n",
    "        self.states = states\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def __str__(self):\n",
    "        value = \"[\\n\"\n",
    "        for i, states_row in enumerate(self.states):\n",
    "            value += \"\\t[\\n\"\n",
    "            for j, state in enumerate(states_row):\n",
    "                value += f\"\\t\\t{str(state)}{',' if j == len(states_row) -1 else ''}\\n\"\n",
    "            value += f\"\\t]{',' if i == len(self.states) -1 else ''}\\n\"\n",
    "\n",
    "        return f\"{self.states}\"\n",
    "\n",
    "    def get_state(self, x: int, y: int):\n",
    "        return self.states[y][x]\n",
    "    \n",
    "    def get_reward(self, state: State, action: Action):\n",
    "        next_x, next_y = action.get_new_position(x=state.y, y=state.y)\n",
    "        return self.rewards[next_y][next_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b202a",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8562bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, mdp: MDP):\n",
    "        self.mdp = mdp\n",
    "    \n",
    "    def train(self, start_x: int, start_y: int, max_steps: int, num_runs: int, discount: int = 0):\n",
    "        state = self.mdp.get_state(start_x, start_y)\n",
    "        actions = state.actions\n",
    "        \n",
    "        print([str(action) for action in actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf8762",
   "metadata": {},
   "source": [
    "### World Map\n",
    "\n",
    "- 7x7 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cab053fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UP', 'RIGHT']\n"
     ]
    }
   ],
   "source": [
    "world = [[ 1, 0, 0, 0, 0, 0, 0],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0 ],\n",
    "        [ -1, -1, -1, -1, -1, -1, 0 ],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0 ],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0 ],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0 ],\n",
    "        [ 0, 0, 0, 0, 0, 0, 0 ]]\n",
    "\n",
    "\n",
    "rewards = np.full((7, 7), -1)\n",
    "rewards[0][0] = 20\n",
    "\n",
    "states = get_states_from_world(world)\n",
    "\n",
    "mdp = MDP(states, rewards)\n",
    "agent = Agent(mdp)\n",
    "agent.train(0, 6, max_steps=50, num_runs=20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "faade8e7af46052c07bca6f5ff0845d7425bd89182e4a8c58edae0657a1fed45"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "faade8e7af46052c07bca6f5ff0845d7425bd89182e4a8c58edae0657a1fed45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
